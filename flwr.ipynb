{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you're on Python > 3.8\n",
    "# !pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Pre Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir '.kaggle'\n",
    "!mkdir '.kaggle/data'\n",
    "\n",
    "with open(\".kaggle/kaggle.json\", 'a+') as f:\n",
    "    f.write('{\"username\":\"rajaxarcmu\",\"key\":\"68d40c5e38e1c786ab57736bc5c9b2cb\"}')\n",
    "    \n",
    "!chmod 600 '.kaggle/kaggle.json'\n",
    "!kaggle datasets download -d 'danofer/compass'\n",
    "!unzip -qo compass.zip -d '.kaggle/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls .kaggle/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('.kaggle/data/propublicaCompassRecividism_data_fairml.csv/propublica_data_for_fairml.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['caucasian'] = ((df['African_American'] + df['Asian'] + df['Hispanic'] + df['Native_American'] + df['Other']) == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 5\n",
    "# REPRESENTS SILO'D ORGANIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from flwr_datasets.partitioner import DirichletPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(df, test_size=0.2)\n",
    "batch_size = 32\n",
    "\n",
    "ds = Dataset.from_pandas(trainset)\n",
    "\n",
    "partitioner = DirichletPartitioner(\n",
    "    num_partitions=NUM_CLIENTS,\n",
    "    partition_by=\"caucasian\",\n",
    "    alpha=1,\n",
    "    min_partition_size=(len(trainset) // (4 * NUM_CLIENTS)),\n",
    "    self_balancing=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "partitioner.dataset = ds\n",
    "datasets = []\n",
    "for i in range(NUM_CLIENTS):\n",
    "    curr_partition = partitioner.load_partition(i)\n",
    "    datasets.append(curr_partition.to_pandas())\n",
    "\n",
    "train_loaders = []\n",
    "val_loaders = []\n",
    "\n",
    "feature_columns = ['Number_of_Priors', 'score_factor','Age_Above_FourtyFive', 'Age_Below_TwentyFive', 'Misdemeanor']\n",
    "\n",
    "for ds in datasets:\n",
    "    train_x = ds[feature_columns].values\n",
    "    train_y = ds['Two_yr_Recidivism'].values\n",
    "    sensitive_feature = ds['caucasian'].values\n",
    "\n",
    "    train_x, val_x, train_y, val_y, sensitive_train, sensitive_val = train_test_split(\n",
    "        train_x, train_y, sensitive_feature, test_size=0.25, shuffle=True, stratify=train_y, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_x_tensor = torch.from_numpy(train_x).float()\n",
    "    train_y_tensor = torch.from_numpy(train_y).float()\n",
    "    sensitive_train_tensor = torch.from_numpy(sensitive_train).float()\n",
    "\n",
    "    valid_x_tensor = torch.from_numpy(val_x).float()\n",
    "    valid_y_tensor = torch.from_numpy(val_y).float()\n",
    "    sensitive_val_tensor = torch.from_numpy(sensitive_val).float()\n",
    "\n",
    "    # Create TensorDataset and DataLoader, including the sensitive attribute\n",
    "    train_dataset = TensorDataset(train_x_tensor, train_y_tensor, sensitive_train_tensor)\n",
    "    valid_dataset = TensorDataset(valid_x_tensor, valid_y_tensor, sensitive_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "    train_loaders.append(train_loader)\n",
    "    val_loaders.append(val_loader)\n",
    "\n",
    "# For test data\n",
    "test_x = testset[feature_columns].values\n",
    "test_y = testset['Two_yr_Recidivism'].values\n",
    "sensitive_test = testset['caucasian'].values\n",
    "\n",
    "test_x_tensor = torch.from_numpy(test_x).float()\n",
    "test_y_tensor = torch.from_numpy(test_y).float()\n",
    "sensitive_test_tensor = torch.from_numpy(sensitive_test).float()\n",
    "\n",
    "test_dataset = TensorDataset(test_x_tensor, test_y_tensor, sensitive_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "def compute_eod(preds, labels, sensitive_feature):\n",
    "    preds_binary = (preds >= 0.5).float()\n",
    "    y_true_mask = (labels == 1).view(-1)\n",
    "\n",
    "    p_a0 = preds_binary[y_true_mask & (sensitive_feature == 0)].mean().item()\n",
    "    p_a1 = preds_binary[y_true_mask & (sensitive_feature == 1)].mean().item()\n",
    "\n",
    "    eod = p_a0 - p_a1\n",
    "    return eod\n",
    "\n",
    "def train(net, trainloader, epochs, verbose=True):\n",
    "    \"\"\"\n",
    "    Train Network on Training Set\n",
    "    \"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        all_preds, all_labels, all_sensitives = [], [], []\n",
    "        \n",
    "        for inputs, labels, sensitive_features in trainloader:\n",
    "            inputs, labels, sensitive_features = inputs.to(DEVICE), labels.to(DEVICE), sensitive_features.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            labels = labels.view(-1, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            predicted = (outputs >= 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Append predictions and sensitive data for EOD computation\n",
    "            all_preds.append(outputs.detach().cpu())\n",
    "            all_labels.append(labels.detach().cpu())\n",
    "            all_sensitives.append(sensitive_features.cpu())\n",
    "        \n",
    "        # Compute EOD at the end of the epoch\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_sensitives = torch.cat(all_sensitives)\n",
    "        \n",
    "        eod = compute_eod(all_preds, all_labels, all_sensitives)\n",
    "        \n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f} - EOD: {eod:.4f}\")\n",
    "\n",
    "def test(net, testloader, verbose=True):\n",
    "    criterion = nn.BCELoss()\n",
    "    net.eval()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    all_preds, all_labels, all_sensitives = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, sensitive_features in testloader:\n",
    "            inputs, labels, sensitive_features = inputs.to(DEVICE), labels.to(DEVICE), sensitive_features.to(DEVICE)\n",
    "            outputs = net(inputs)\n",
    "            labels = labels.view(-1, 1)\n",
    "            loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            predicted = (outputs >= 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Append predictions and sensitive data for EOD computation\n",
    "            all_preds.append(outputs.detach().cpu())\n",
    "            all_labels.append(labels.detach().cpu())\n",
    "            all_sensitives.append(sensitive_features.cpu())\n",
    "    \n",
    "    # Compute EOD at the end of testing\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_sensitives = torch.cat(all_sensitives)\n",
    "    \n",
    "    eod = compute_eod(all_preds, all_labels, all_sensitives)\n",
    "    \n",
    "    loss /= len(testloader.dataset)\n",
    "    acc = correct / total\n",
    "    if verbose:\n",
    "        print(f\"Test Loss: {loss:.4f} - Acc: {acc:.4f} - EOD: {eod:.4f}\")\n",
    "    return loss, acc, eod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_CLIENTS):\n",
    "    train_loader = train_loaders[i]\n",
    "    val_loader = val_loaders[i]\n",
    "    model = model.to(DEVICE)\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(model, train_loader, 1, verbose=False)\n",
    "        loss, acc, eod = test(model, val_loader, verbose=False)\n",
    "\n",
    "    loss, acc, eod = test(model, test_loader, verbose=False)\n",
    "    print(f\"Client {i} - Test Loss: {loss:.4f} - Acc: {acc:.4f} - EOD: {eod:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning with Flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from custom_flwr.server_app import server_fn as server_fn_custom\n",
    "from custom_flwr.client_app import client_fn as client_fn_custom\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    context.run_config = {\n",
    "        'num-server-rounds' : 10,\n",
    "        'fraction-fit': 1,\n",
    "        'fraction-evaluate': 1,\n",
    "        # 'local-epochs': 2,\n",
    "        'server-device': str(DEVICE),\n",
    "        'use-wandb': False\n",
    "    }\n",
    "    return server_fn_custom(context)\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    return client_fn_custom(context)\n",
    "\n",
    "client = ClientApp(client_fn=client_fn)\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "\n",
    "backend_config = {\"client_resources\": None}\n",
    "\n",
    "NUM_PARTITIONS = 5\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_PARTITIONS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hi"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
