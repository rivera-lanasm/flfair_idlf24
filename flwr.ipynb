{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you're on Python > 3.8\n",
    "# !pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import logging \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "\n",
    "from datasets import Dataset\n",
    "from flwr_datasets.partitioner import DirichletPartitioner\n",
    "\n",
    "from flwr_datasets.visualization import plot_label_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # ablation name\n",
    "    \"name\":\"ablation_0\",\n",
    "    # sensitive feature in X --> caucasian\n",
    "    \"sensitive_cat\": \"caucasian\",\n",
    "    # outcome, y --> Two_yr_Recidivism, Age_Below_TwentyFive\n",
    "    \"outcome\": \"Two_yr_Recidivism\",\n",
    "    # alpha - data heterogenity level 500, 5, .5\n",
    "    \"alpha\": 5,\n",
    "    # beta - weight on fairness metric, .05, 0.1, .15\n",
    "    \"beta\": 0.1,\n",
    "    # gamma - weight on ind. fairness vs group, 0, .25, .5\n",
    "    \"gamma\" : 0,\n",
    "    # num clients\n",
    "    \"num_clients\": 5,\n",
    "    # local epochs\n",
    "    \"local_epochs\":2\n",
    "    \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/{}.yaml\".format(config[\"name\"]), \"w\") as file:\n",
    "    yaml.dump(config, file, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"ablation_log/{}.log\".format(config[\"name\"]),  # Log file name\n",
    "    level=logging.INFO,       # Logging level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"  # Log format\n",
    ")\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.info(\"init ablation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir '.kaggle'\n",
    "# !mkdir '.kaggle/data'\n",
    "\n",
    "# with open(\".kaggle/kaggle.json\", 'a+') as f:\n",
    "#     f.write('{\"username\":\"rajaxarcmu\",\"key\":\"68d40c5e38e1c786ab57736bc5c9b2cb\"}')\n",
    "    \n",
    "# !chmod 600 '.kaggle/kaggle.json'\n",
    "# !kaggle datasets download -d 'danofer/compass'\n",
    "# !unzip -qo compass.zip -d '.kaggle/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls .kaggle/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lik = round(df['Age_Below_TwentyFive'].sum()/len(df), 3)\n",
    "outcome = config[\"outcome\"]\n",
    "logger.info(f\"Baseline likelihood of {outcome} {baseline_lik}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Client Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('.kaggle/data/propublicaCompassRecividism_data_fairml.csv/propublica_data_for_fairml.csv')\n",
    "df['caucasian'] = ((df['African_American'] + df['Asian'] + df['Hispanic'] + df['Native_American'] + df['Other']) == 0).astype(int)\n",
    "\n",
    "baseline_lik = round(df['Age_Below_TwentyFive'].sum()/len(df), 3)\n",
    "outcome = config[\"outcome\"]\n",
    "logger.info(f\"Baseline likelihood of {outcome} {baseline_lik}\")\n",
    "\n",
    "trainset, testset = train_test_split(df, test_size=0.2)\n",
    "\n",
    "ds = Dataset.from_pandas(trainset)\n",
    "\n",
    "min_partition_size = (len(trainset) // (2*config[\"num_clients\"]))\n",
    "logger.info(f\"min_partition_size {min_partition_size}\")\n",
    "\n",
    "partitioner = DirichletPartitioner(\n",
    "    num_partitions=config[\"num_clients\"],\n",
    "    partition_by=config[\"sensitive_cat\"],\n",
    "    alpha=config[\"alpha\"],\n",
    "    min_partition_size=min_partition_size, \n",
    "    self_balancing=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "partitioner.dataset = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, df_viz = plot_label_distributions(\n",
    "    partitioner,\n",
    "    label_name=\"caucasian\",\n",
    "    plot_type=\"bar\",\n",
    "    size_unit=\"absolute\",\n",
    "    partition_id_axis=\"x\",\n",
    "    legend=True,\n",
    "    verbose_labels=True,\n",
    "    title=\"Per Partition Labels Distribution\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_flwr.task import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset, testset = train_test_split(df, test_size=0.2)\n",
    "# batch_size = 32\n",
    "\n",
    "# ds = Dataset.from_pandas(trainset)\n",
    "\n",
    "# partitioner = DirichletPartitioner(\n",
    "#     num_partitions=config[\"num_clients\"],\n",
    "#     partition_by=config[\"sensitive_cat\"],\n",
    "#     alpha=config[\"alpha\"],\n",
    "#     min_partition_size=(len(trainset) // (2 * config[\"num_clients\"])),\n",
    "#     self_balancing=True,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# partitioner.dataset = ds\n",
    "# datasets = []\n",
    "# for i in range(config[\"num_clients\"]):\n",
    "#     curr_partition = partitioner.load_partition(i)\n",
    "#     datasets.append(curr_partition.to_pandas())\n",
    "\n",
    "# train_loaders = []\n",
    "# val_loaders = []\n",
    "\n",
    "# feature_columns = ['Number_of_Priors', 'score_factor','Age_Above_FourtyFive', 'Age_Below_TwentyFive', 'Misdemeanor']\n",
    "\n",
    "# for ds in datasets:\n",
    "#     train_x = ds[feature_columns].values\n",
    "#     train_y = ds['Two_yr_Recidivism'].values\n",
    "#     sensitive_feature = ds['caucasian'].values\n",
    "\n",
    "#     train_x, val_x, train_y, val_y, sensitive_train, sensitive_val = train_test_split(\n",
    "#         train_x, train_y, sensitive_feature, test_size=0.25, shuffle=True, stratify=train_y, random_state=42\n",
    "#     )\n",
    "    \n",
    "#     train_x_tensor = torch.from_numpy(train_x).float()\n",
    "#     train_y_tensor = torch.from_numpy(train_y).float()\n",
    "#     sensitive_train_tensor = torch.from_numpy(sensitive_train).float()\n",
    "\n",
    "#     valid_x_tensor = torch.from_numpy(val_x).float()\n",
    "#     valid_y_tensor = torch.from_numpy(val_y).float()\n",
    "#     sensitive_val_tensor = torch.from_numpy(sensitive_val).float()\n",
    "\n",
    "#     # Create TensorDataset and DataLoader, including the sensitive attribute\n",
    "#     train_dataset = TensorDataset(train_x_tensor, train_y_tensor, sensitive_train_tensor)\n",
    "#     valid_dataset = TensorDataset(valid_x_tensor, valid_y_tensor, sensitive_val_tensor)\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "#     train_loaders.append(train_loader)\n",
    "#     val_loaders.append(val_loader)\n",
    "\n",
    "# # For test data\n",
    "# test_x = testset[feature_columns].values\n",
    "# test_y = testset['Two_yr_Recidivism'].values\n",
    "# sensitive_test = testset['caucasian'].values\n",
    "\n",
    "# test_x_tensor = torch.from_numpy(test_x).float()\n",
    "# test_y_tensor = torch.from_numpy(test_y).float()\n",
    "# sensitive_test_tensor = torch.from_numpy(sensitive_test).float()\n",
    "\n",
    "# test_dataset = TensorDataset(test_x_tensor, test_y_tensor, sensitive_test_tensor)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders, val_loaders, test_loader = load_data(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolated Client Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "    def compute_indfair(self, preds, labels):\n",
    "        # Convert lists to tensors if they aren't already\n",
    "        if isinstance(preds, list):\n",
    "            preds = torch.cat(preds)\n",
    "        if isinstance(labels, list):\n",
    "            labels = torch.cat(labels)\n",
    "    \n",
    "        # Group predictions by the actual labels\n",
    "        preds_0 = preds[labels == 0]\n",
    "        preds_1 = preds[labels == 1]\n",
    "    \n",
    "        # Compute mean predictions for each group\n",
    "        mean_preds_0 = preds_0.mean().item()\n",
    "        mean_preds_1 = preds_1.mean().item()\n",
    "    \n",
    "        # Compute squared differences\n",
    "        squared_diff_0 = ((preds_0 - mean_preds_0) ** 2).mean().item()\n",
    "        squared_diff_1 = ((preds_1 - mean_preds_1) ** 2).mean().item()\n",
    "    \n",
    "        # Average the squared differences\n",
    "        avg_squared_diff = (squared_diff_0 + squared_diff_1) / 2\n",
    "        return avg_squared_diff\n",
    "    \n",
    "    def compute_eod(self, preds, labels, sensitive_feature):\n",
    "        preds_binary = (preds >= 0.5).float()\n",
    "        y_true_mask = (labels == 1).view(-1)\n",
    "    \n",
    "        p_a0 = preds_binary[y_true_mask & (sensitive_feature == 0)].mean().item()\n",
    "        p_a1 = preds_binary[y_true_mask & (sensitive_feature == 1)].mean().item()\n",
    "    \n",
    "        eod = p_a0 - p_a1\n",
    "        return eod\n",
    "    \n",
    "    def model_train(self, net, trainloader, epochs, verbose=True):\n",
    "        \"\"\"\n",
    "        Train Network on Training Set\n",
    "        \"\"\"\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(net.parameters())\n",
    "        net.train()\n",
    "        for epoch in range(epochs):\n",
    "            correct, total, epoch_loss = 0, 0, 0.0\n",
    "            all_preds, all_labels, all_sensitives = [], [], []\n",
    "            \n",
    "            for inputs, labels, sensitive_features in trainloader:\n",
    "                inputs, labels, sensitive_features = inputs.to(DEVICE), labels.to(DEVICE), sensitive_features.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                labels = labels.view(-1, 1)\n",
    "                # loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                # optimizer step\n",
    "                optimizer.step()\n",
    "                # epoch loss increemnt\n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "                # predicted\n",
    "                predicted = (outputs >= 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                # prop correct\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Append predictions and sensitive data for EOD computation\n",
    "                all_preds.append(outputs.detach().cpu())\n",
    "                all_labels.append(labels.detach().cpu())\n",
    "                all_sensitives.append(sensitive_features.cpu())\n",
    "            \n",
    "            # Compute EOD at the end of the epoch\n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_labels = torch.cat(all_labels)\n",
    "            all_sensitives = torch.cat(all_sensitives)\n",
    "            \n",
    "            eod = self.compute_eod(all_preds, all_labels, all_sensitives)\n",
    "            \n",
    "            epoch_loss /= len(trainloader.dataset)\n",
    "            epoch_acc = correct / total\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f} - EOD: {eod:.4f}\")\n",
    "    \n",
    "    def model_test(self, net, testloader, verbose=True):\n",
    "        criterion = nn.BCELoss()\n",
    "        net.eval()\n",
    "        correct, total, loss = 0, 0, 0.0\n",
    "        all_preds, all_labels, all_sensitives = [], [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, sensitive_features in testloader:\n",
    "                inputs, labels, sensitive_features = inputs.to(DEVICE), labels.to(DEVICE), sensitive_features.to(DEVICE)\n",
    "                outputs = net(inputs)\n",
    "                labels = labels.view(-1, 1)\n",
    "                loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "                predicted = (outputs >= 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Append predictions and sensitive data for EOD computation\n",
    "                all_preds.append(outputs.detach().cpu())\n",
    "                all_labels.append(labels.detach().cpu())\n",
    "                all_sensitives.append(sensitive_features.cpu())\n",
    "        \n",
    "        # Compute EOD at the end of testing\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_sensitives = torch.cat(all_sensitives)\n",
    "        \n",
    "        eod = self.compute_eod(all_preds, all_labels, all_sensitives)\n",
    "        \n",
    "        loss /= len(testloader.dataset)\n",
    "        acc = correct / total\n",
    "        if verbose:\n",
    "            print(f\"Test Loss: {loss:.4f} - Acc: {acc:.4f} - EOD: {eod:.4f}\")\n",
    "        return loss, acc, eod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized Learning - Isolated Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_avg = 0\n",
    "curr_eod = 0\n",
    "\n",
    "max_eod = -1\n",
    "min_acc = 1\n",
    "\n",
    "for i in range(config[\"num_clients\"]):\n",
    "    train_loader = train_loaders[i]\n",
    "    val_loader = val_loaders[i]\n",
    "    model = model.to(DEVICE)\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.model_train(model, train_loader, 1, verbose=False)\n",
    "        loss, acc, eod = model.model_test(model, val_loader, verbose=False)\n",
    "\n",
    "    loss, acc, eod = model.model_test(model, test_loader, verbose=False)\n",
    "\n",
    "    print(f\"Client {i} - Test Loss: {loss:.4f} - Acc: {acc:.4f} - EOD: {eod:.4f}\")\n",
    "    \n",
    "    curr_eod += eod\n",
    "    curr_avg += acc\n",
    "\n",
    "    if eod > max_eod:\n",
    "        max_eod = eod\n",
    "    if acc < min_acc:\n",
    "        min_acc = acc\n",
    "\n",
    "num_clients = config[\"num_clients\"]\n",
    "\n",
    "print(\"---\")\n",
    "print(f\"Average EOD: {curr_eod / num_clients:.4f}\")\n",
    "print(f\"Average Accuracy: {curr_avg / num_clients:.4f}\")\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "print(f\"Max EOD: {max_eod:.4f}\")\n",
    "print(f\"Min Accuracy: {min_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(config[\"num_clients\"]):\n",
    "#     train_loader = train_loaders[i]\n",
    "#     val_loader = val_loaders[i]\n",
    "#     model = model.to(DEVICE)\n",
    "#     epochs = 10\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.model_train(model, train_loader, 1, verbose=False)\n",
    "#         loss, acc, eod = model.model_test(model, val_loader, verbose=False)\n",
    "\n",
    "#     loss, acc, eod = model.model_test(model, test_loader, verbose=False)\n",
    "#     print(f\"Client {i} - Test Loss: {loss:.4f} - Acc: {acc:.4f} - EOD: {eod:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning with Flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from custom_flwr.server_app import server_fn as server_fn_custom\n",
    "from custom_flwr.client_app import client_fn as client_fn_custom\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    context.run_config = {\n",
    "        'num-server-rounds' : 2,\n",
    "        'fraction-fit': 1,\n",
    "        'fraction-evaluate': 1,\n",
    "        # 'local-epochs': 2,\n",
    "        'server-device': str(DEVICE),\n",
    "        'use-wandb': False,\n",
    "        'beta':config[\"beta\"],\n",
    "        \"gamma\":config[\"gamma\"],\n",
    "        \"config\":config\n",
    "    }\n",
    "    return server_fn_custom(context)\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    return client_fn_custom(context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_config = {\"client_resources\": None}\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=config[\"num_clients\"],s\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
