{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rivlanm/miniconda3/envs/idlf24/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-06 11:57:35,108\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "\n",
    "from datasets import Dataset\n",
    "from flwr_datasets.partitioner import DirichletPartitioner\n",
    "\n",
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # flower config \n",
    "    \"num_clients\":10,\n",
    "    # train data \n",
    "    \"train_test_split\":.2,\n",
    "    # client model\n",
    "    \"batch_size\":32,\n",
    "\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Pre Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘.kaggle’: File exists\n",
      "mkdir: cannot create directory ‘.kaggle/data’: File exists\n",
      "compass.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "compas-scores-raw.csv\tcox-violent-parsed_filt.csv\n",
      "cox-violent-parsed.csv\tpropublicaCompassRecividism_data_fairml.csv\n"
     ]
    }
   ],
   "source": [
    "!mkdir '.kaggle'\n",
    "!mkdir '.kaggle/data'\n",
    "\n",
    "with open(\".kaggle/kaggle.json\", 'a+') as f:\n",
    "    f.write('{\"username\":\"rajaxarcmu\",\"key\":\"68d40c5e38e1c786ab57736bc5c9b2cb\"}')\n",
    "    \n",
    "!chmod 600 '.kaggle/kaggle.json'\n",
    "!kaggle datasets download -d 'danofer/compass'\n",
    "!unzip -qo compass.zip -d '.kaggle/data'\n",
    "\n",
    "!ls .kaggle/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6172, 12)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('.kaggle/data/propublicaCompassRecividism_data_fairml.csv/propublica_data_for_fairml.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['caucasian'] = ((df['African_American'] + df['Asian'] + df['Hispanic'] + df['Native_American'] + df['Other']) == 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Two_yr_Recidivism</th>\n",
       "      <th>Number_of_Priors</th>\n",
       "      <th>score_factor</th>\n",
       "      <th>Age_Above_FourtyFive</th>\n",
       "      <th>Age_Below_TwentyFive</th>\n",
       "      <th>African_American</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>Native_American</th>\n",
       "      <th>Other</th>\n",
       "      <th>Female</th>\n",
       "      <th>Misdemeanor</th>\n",
       "      <th>caucasian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Two_yr_Recidivism  Number_of_Priors  score_factor  Age_Above_FourtyFive  \\\n",
       "0                  0                 0             0                     1   \n",
       "1                  1                 0             0                     0   \n",
       "2                  1                 4             0                     0   \n",
       "3                  0                 0             0                     0   \n",
       "4                  1                14             1                     0   \n",
       "\n",
       "   Age_Below_TwentyFive  African_American  Asian  Hispanic  Native_American  \\\n",
       "0                     0                 0      0         0                0   \n",
       "1                     0                 1      0         0                0   \n",
       "2                     1                 1      0         0                0   \n",
       "3                     0                 0      0         0                0   \n",
       "4                     0                 0      0         0                0   \n",
       "\n",
       "   Other  Female  Misdemeanor  caucasian  \n",
       "0      1       0            0          0  \n",
       "1      0       0            0          0  \n",
       "2      0       0            0          0  \n",
       "3      1       0            1          0  \n",
       "4      0       0            0          1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Data Loader\n",
    "- including data heterogeneity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rivlanm/miniconda3/envs/idlf24/lib/python3.8/site-packages/flwr_datasets/partitioner/dirichlet_partitioner.py:300: UserWarning: The specified min_partition_size (123) was not satisfied for alpha ([0.5]) after 0 attempts at sampling from the Dirichlet distribution. The probability sampling from the Dirichlet distribution will be repeated. Note: This is not a desired behavior. It is recommended to adjust the alpha or min_partition_size instead. Generating partitions by sampling from a list of very wide range of alpha values can be hard to achieve. Try reducing the range between maximum (0.5) and minimum alpha (0.5) values or increasing all the values.\n",
      "  warnings.warn(\n",
      "/home/rivlanm/miniconda3/envs/idlf24/lib/python3.8/site-packages/flwr_datasets/partitioner/dirichlet_partitioner.py:300: UserWarning: The specified min_partition_size (123) was not satisfied for alpha ([0.5]) after 1 attempts at sampling from the Dirichlet distribution. The probability sampling from the Dirichlet distribution will be repeated. Note: This is not a desired behavior. It is recommended to adjust the alpha or min_partition_size instead. Generating partitions by sampling from a list of very wide range of alpha values can be hard to achieve. Try reducing the range between maximum (0.5) and minimum alpha (0.5) values or increasing all the values.\n",
      "  warnings.warn(\n",
      "/home/rivlanm/miniconda3/envs/idlf24/lib/python3.8/site-packages/flwr_datasets/partitioner/dirichlet_partitioner.py:300: UserWarning: The specified min_partition_size (123) was not satisfied for alpha ([0.5]) after 2 attempts at sampling from the Dirichlet distribution. The probability sampling from the Dirichlet distribution will be repeated. Note: This is not a desired behavior. It is recommended to adjust the alpha or min_partition_size instead. Generating partitions by sampling from a list of very wide range of alpha values can be hard to achieve. Try reducing the range between maximum (0.5) and minimum alpha (0.5) values or increasing all the values.\n",
      "  warnings.warn(\n",
      "/home/rivlanm/miniconda3/envs/idlf24/lib/python3.8/site-packages/flwr_datasets/partitioner/dirichlet_partitioner.py:300: UserWarning: The specified min_partition_size (123) was not satisfied for alpha ([0.5]) after 3 attempts at sampling from the Dirichlet distribution. The probability sampling from the Dirichlet distribution will be repeated. Note: This is not a desired behavior. It is recommended to adjust the alpha or min_partition_size instead. Generating partitions by sampling from a list of very wide range of alpha values can be hard to achieve. Try reducing the range between maximum (0.5) and minimum alpha (0.5) values or increasing all the values.\n",
      "  warnings.warn(\n",
      "/home/rivlanm/miniconda3/envs/idlf24/lib/python3.8/site-packages/flwr_datasets/partitioner/dirichlet_partitioner.py:300: UserWarning: The specified min_partition_size (123) was not satisfied for alpha ([0.5]) after 4 attempts at sampling from the Dirichlet distribution. The probability sampling from the Dirichlet distribution will be repeated. Note: This is not a desired behavior. It is recommended to adjust the alpha or min_partition_size instead. Generating partitions by sampling from a list of very wide range of alpha values can be hard to achieve. Try reducing the range between maximum (0.5) and minimum alpha (0.5) values or increasing all the values.\n",
      "  warnings.warn(\n",
      "/home/rivlanm/miniconda3/envs/idlf24/lib/python3.8/site-packages/flwr_datasets/partitioner/dirichlet_partitioner.py:300: UserWarning: The specified min_partition_size (123) was not satisfied for alpha ([0.5]) after 5 attempts at sampling from the Dirichlet distribution. The probability sampling from the Dirichlet distribution will be repeated. Note: This is not a desired behavior. It is recommended to adjust the alpha or min_partition_size instead. Generating partitions by sampling from a list of very wide range of alpha values can be hard to achieve. Try reducing the range between maximum (0.5) and minimum alpha (0.5) values or increasing all the values.\n",
      "  warnings.warn(\n",
      "/home/rivlanm/miniconda3/envs/idlf24/lib/python3.8/site-packages/flwr_datasets/partitioner/dirichlet_partitioner.py:300: UserWarning: The specified min_partition_size (123) was not satisfied for alpha ([0.5]) after 6 attempts at sampling from the Dirichlet distribution. The probability sampling from the Dirichlet distribution will be repeated. Note: This is not a desired behavior. It is recommended to adjust the alpha or min_partition_size instead. Generating partitions by sampling from a list of very wide range of alpha values can be hard to achieve. Try reducing the range between maximum (0.5) and minimum alpha (0.5) values or increasing all the values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train/test split \n",
    "trainset, testset = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# train data set\n",
    "ds = Dataset.from_pandas(trainset)\n",
    "\n",
    "# Flower Partitioner\n",
    "partitioner = DirichletPartitioner(\n",
    "    num_partitions=config[\"num_clients\"],\n",
    "    partition_by=\"caucasian\",\n",
    "    alpha=0.5,\n",
    "    min_partition_size=(len(trainset) // (4 * config[\"num_clients\"])),\n",
    "    self_balancing=True,\n",
    "    shuffle=True)\n",
    "\n",
    "# populate train partitioned datasets \n",
    "partitioner.dataset = ds\n",
    "datasets = []\n",
    "for i in range(config[\"num_clients\"]):\n",
    "    curr_partition = partitioner.load_partition(i)\n",
    "    datasets.append(curr_partition.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate train/test data loaders \n",
    "train_loaders = []\n",
    "val_loaders = []\n",
    "\n",
    "feature_columns = ['Number_of_Priors', 'score_factor','Age_Above_FourtyFive', 'Age_Below_TwentyFive', 'Misdemeanor']\n",
    "\n",
    "for ds in datasets:\n",
    "    train_x = ds[feature_columns].values\n",
    "    train_y = ds['Two_yr_Recidivism'].values\n",
    "    sensitive_feature = ds['caucasian'].values\n",
    "\n",
    "    train_x, val_x, train_y, val_y, sensitive_train, sensitive_val = train_test_split(\n",
    "        train_x, train_y, sensitive_feature, test_size=0.25, shuffle=True, stratify=train_y, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_x_tensor = torch.from_numpy(train_x).float()\n",
    "    train_y_tensor = torch.from_numpy(train_y).float()\n",
    "    sensitive_train_tensor = torch.from_numpy(sensitive_train).float()\n",
    "\n",
    "    valid_x_tensor = torch.from_numpy(val_x).float()\n",
    "    valid_y_tensor = torch.from_numpy(val_y).float()\n",
    "    sensitive_val_tensor = torch.from_numpy(sensitive_val).float()\n",
    "\n",
    "    # Create TensorDataset and DataLoader, including the sensitive attribute\n",
    "    train_dataset = TensorDataset(train_x_tensor, train_y_tensor, sensitive_train_tensor)\n",
    "    valid_dataset = TensorDataset(valid_x_tensor, valid_y_tensor, sensitive_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(valid_dataset, batch_size=config[\"batch_size\"])\n",
    "\n",
    "    train_loaders.append(train_loader)\n",
    "    val_loaders.append(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build test data set\n",
    "test_x = testset[feature_columns].values\n",
    "test_y = testset['Two_yr_Recidivism'].values\n",
    "sensitive_test = testset['caucasian'].values\n",
    "\n",
    "test_x_tensor = torch.from_numpy(test_x).float()\n",
    "test_y_tensor = torch.from_numpy(test_y).float()\n",
    "sensitive_test_tensor = torch.from_numpy(sensitive_test).float()\n",
    "\n",
    "test_dataset = TensorDataset(test_x_tensor, test_y_tensor, sensitive_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
